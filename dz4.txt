1.Расскажите, как работает регуляризация в решающих деревьях, какие параметры мы штрафуем в данных алгоритмах?
Переобучение часто возникает при попытках улучшения datascience-моделей. Чем больше параметров, тем проще совершить эту ошибку.
Регуляризация для деревьев принятия решений
Регуляризация применима не только к линейной регрессии. Это набор приемов к разным моделям, ограничивающий их в стремлении к переобучению.
В семье моделей на основе деревьев принятия решений одно дерево способно выучить все данные. Это приводит к сильному переобучению. 
Поэтому стратегии регуляризации для них встроены в большинство популярных пакетов.
 Эти стратегии часто заключаются в ограничении определенных параметров дерева:

Глубина дерева — параметр, который ограничивает максимальный рост дерева (деревья принятия решений растут в глубину).
 Этот параметр позволяет уменьшить переобучение, но ограничивает количество переменных для каждого конкретного листа.
Минимальный вес листа — параметр, который ограничивает рост дерева, когда следующее деление листа приводит к тому, что хотя бы в одном из них слишком мало наблюдений, что делало бы его слишком специфичным.
Также для регрессионных деревьев можно настраивать коэффициенты для линейной регуляризации, которые используются в их листах.

Леса деревьев принятия решений — более сложная модель. Для нее настраиваются параметры регуляризации, которые встречаются во многих нейросетях и являются универсальными для большинства итеративных моделей обучения:
Скорость обучения (learning rate) — коэффициент, который показывает, насколько подробно нужно уточнять свои результаты с каждым шагом. Если сделать его слишком низким, то понадобится больше итераций, чтобы прийти к хорошему решению. 
Но есть риск, что остановиться вовремя не получится, и модель выучит данные слишком подробно, что приведет к переобучению. Если же, напротив, этот параметр будет слишком высоким, модель сможет выйти на хорошую точность за меньшее количество итераций. Но ей будет сложно приблизиться к лучшему результату, и она останется недообученной.
Отсев (dropout) — параметр, которым задается относительная часть всех данных, скрытая случайным образом во время обучения.
Скрывая часть данных от моделей, мы отнимаем у них возможность использовать всю вариативность данных, чтоб выучить их наизусть. Слишком высокий отсев может скрыть искомую зависимость между признаками.

Ранняя остановка (early stopping) — это стратегия, при которой мы возвращаемся к последней лучшей итерации в случае, если после нескольких итераций подряд точность модели на скрытых валидационных данных не улучшилась. Это универсальный метод. Чтобы его применять, нужно достаточное количество валидационных данных для обучения.

Регуляризация — это один из таких инструментов; своего рода встроенный предохранитель.
2.По какому принципу рассчитывается "важность признака (feature_importance)" в ансамблях деревьев?
Важность рассчитывается для отдельного дерева решений по величине, на которую каждая точка разделения атрибута улучшает показатель производительности, взвешенный по количеству наблюдений, за которые отвечает узел.
 Показателем производительности может быть чистота (индекс Джини), используемая для выбора точек разделения, или другая более конкретная функция ошибок.