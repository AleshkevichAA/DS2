1. Для чего и в каких случаях полезны различные варианты усреднения для метрик качества классификации: micro, macro, weighted?
macro avg - рассчитывает среднее метрическое показателей для каждого класса, взвешивая их одинаково. 
Проблема с множественной классификацией, не подверженная дисбалансу данных, легко затрагивается категориями с высокой узнаваемостью( высокая отзывчивость, высокая точность).
Макроусреднее учитывает все классы в равной степени. 
weighted avg - рассчитывает среднее метрических оценок, взвешивая их пропорционально представительству в сведениях.
micro avg - применимая среда, мультикатегория несбалансированная, если данные крайне несбалансированы, то это повлияет на результаты. 
Микроусреднее нивелирует влияние маленьких классов, полезно в задачах классификации и фильтрации.

2. В чём разница между моделями xgboost, lightgbm и catboost или какие их основные особенности?
Lightgbm использует технику односторонней выборки на основе градиента(GOSS) для фильтрации экземпляров классов для нахождения
границы разделения, в то время как xgboost использует предварительно отсортированный алгоритм и алгоритм на основе гистограммы для 
вычисления наилучшего разделения.
Catboost обладает гибкостью, позволяя задавать индексы категориальных столбцов, чтобы его можно было кодировать 
как кодирование в одно касание с использованием one_hot_max_size(кодирование в одно касание для всех функций с числом различных значений,
меньших или равным данному значению параметра). Если ничего не передать в аргументе cat_features, CatBoost будет обрабатывать все столбцы
как числовые переменные.
По сравнению с lightgbm и xgboost автоматически преобразуют категориальные признаки в числовые. Использует функции комбинированных категорий,
которые могут использовать взаимосвязь между функциями, что расширяет возможности измерения. Использует метод ранжирования
продвижения для борьбы с точками шума в обучающем наборе, чтобы избежать отклонения оценки градиента и решить проблему смещения предсказания.
В качество базовой модели используется полностью симметричное дерево.
	Lightgbm также может обрабатывать категориальные функции, вводя имена функций. Он не конвертируется в одноразовое
кодирование и намного быстрее. Lightgbm использует специальный алгоритм, чтобы найти значение разделения категориальных признаков.
Перед построением набора данных, надо преобразовать категориальные функции в тип int.
	Xgboost не может обрабатывать категориальные функции сам по себе, он принимает только числовые значения, подобно случайному лесу.
Поэтому перед подачей категориальных данных в Xgboost необходимо выполнить перкодирование.